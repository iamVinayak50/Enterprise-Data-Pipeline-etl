
## ğŸ›  Project: End-to-End ETL Pipeline using Dataform, Dataflow & Airflow

## Event-driven & Scheduled ETL Pipeline on Google Cloud

This project implements a **complete ETL workflow** where:

1. Raw data from **BigQuery (multiple projects)** is transformed via **Dataform** (Bronze â†’ Silver â†’ Gold).  
2. Processed data is then loaded into **MySQL** using **Dataflow (Apache Beam)** pipelines.  
3. The **Airflow DAG** orchestrates the workflow daily, monitors logs, and sends email notifications.  

![flow-diagram](flow-diagram.png)

---

### ğŸ—‚ Project Structure


## ğŸ“ Project Structure

```text
gcp_data_engineering_project/
â”‚
â”œâ”€â”€ dataform/                        # Dataform ETL repository
â”‚   â”œâ”€â”€ definitions/                 # SQLX scripts for transformations
â”‚   â”‚   â”œâ”€â”€ bronze/                  # ğŸ”¹ Raw â†’ Bronze layer transformations (initial cleaning)
â”‚   â”‚   â”œâ”€â”€ silver/                  # ğŸ”¹ Bronze â†’ Silver layer (enriched/processed)
â”‚   â”‚   â”œâ”€â”€ gold/                    # ğŸ”¹ Silver â†’ Gold layer (business-ready tables)
â”‚   â”‚   â””â”€â”€ final/                   # ğŸ”¹ Optional final aggregated views
â”‚   â”œâ”€â”€ includes/                    # ğŸ”¹ Reusable macros/functions for SQL transformations
â”‚   â”œâ”€â”€ workflow_settings.yaml       # ğŸ”¹ Workflow dependencies & task order
â”‚   â”œâ”€â”€ dataform.json                # ğŸ”¹ Project configuration (warehouse, schema)
â”‚   â””â”€â”€ package.json                 # ğŸ”¹ Node.js dependencies for Dataform CLI
â”‚
â”œâ”€â”€ dataflow/                         # Dataflow / Apache Beam pipelines
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ tables_config.py         # ğŸ“‹ BigQuery â†’ MySQL table mapping, primary keys, load type
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ secret_manager.py        # ğŸ” Fetch secrets from GCP Secret Manager
â”‚   â”‚   â””â”€â”€ mysql_utils.py           # ğŸ¬ MySQL connection & SCD1 upsert functions
â”‚   â””â”€â”€ bq_to_mysql_scd1.py          # ğŸŒŠ Main Dataflow pipeline script
â”‚
â”œâ”€â”€ airflow/                          # Airflow DAGs for orchestration
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ etl_gold_to_mysql.py     # ğŸ•‘ DAG triggers Dataform + Dataflow pipelines with email notifications
â”‚
â”œâ”€â”€ cloudbuild.yaml                   # â˜ï¸ Cloud Build CI/CD config for automated deployment
â”œâ”€â”€ requirements.txt                  # ğŸ“¦ Python dependencies (apache-beam, mysql-connector, airflow)
â””â”€â”€ README.md                         # ğŸ“ Project overview & instructions






---

### âœ… Prerequisites

- Python 3.8+  
- Google Cloud SDK installed (`gcloud init`)  
- Google Cloud project with billing enabled  
- Enable the following APIs:
  - Dataform API  
  - Dataflow API  
  - BigQuery API  
  - Secret Manager API  
  - Cloud Composer (Airflow) API  
- MySQL instance available to load data  
- Optional: Cloud Build for CI/CD automation  

---

### âœ… Use Case

- Transform raw BigQuery tables across projects via **Dataform** (Bronze â†’ Silver â†’ Gold)  
- Perform **SCD Type 1** incremental updates to MySQL  
- Schedule pipelines **daily at 12 AM** via **Airflow**  
- Monitor pipeline execution, send email notifications, and maintain logs  

---

### ğŸ”§ Setup Instructions

#### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/iamVinayak50/Enterprise-Data-Pipeline-etl.git
cd gcp_data_engineering_project



### ğŸ‘¨â€ğŸ’» Author

Hi, my name is **Vinayak Shegar**.  
I am a **GCP Data Engineer** with 4 years of experience in building **scalable ETL pipelines** using **Dataform, Dataflow, Airflow, BigQuery, and MySQL**.  

You can reach me via email or connect with me on LinkedIn to discuss GCP data engineering projects, best practices, or collaborations! ğŸš€  

![Coding GIF](https://media.giphy.com/media/l0MYt5jPR6QX5pnqM/giphy.gif)

